#!/usr/bin/env python3
"""
Test parsing and chunking without Neo4j
"""
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from src.parsers.clause_extractor import ClauseExtractor
from src.chunking.semantic_chunker import SemanticChunker

# Sample insurance policy text
SAMPLE_TEXT = """
ã€ë„ë‚œìœ„í—˜ íŠ¹ë³„ì•½ê´€ã€‘

ì œ1ì¡°(ë³´ìƒí•˜ëŠ” ì†í•´) íšŒì‚¬ëŠ” ë³´í—˜ì¦ê¶Œì— ê¸°ì¬ëœ ë³´í—˜ì˜ ëª©ì ì— ëŒ€í•˜ì—¬ ë„ë‚œìœ¼ë¡œ ì¸í•œ ì†í•´ë¥¼ ë³´ìƒí•©ë‹ˆë‹¤.
â‘  ë„ë‚œìœ¼ë¡œ ì¸í•œ ì§ì ‘ì ì¸ ì†í•´ë¥¼ ë³´ìƒí•©ë‹ˆë‹¤.
â‘¡ ë„ë‚œë¬¼í’ˆì˜ íšŒìˆ˜ì— ì†Œìš”ëœ ë¹„ìš©ì„ ë³´ìƒí•©ë‹ˆë‹¤.

ì œ2ì¡°(ë³´ìƒí•˜ì§€ ì•„ë‹ˆí•˜ëŠ” ì†í•´) íšŒì‚¬ëŠ” ë‹¤ìŒì˜ ì†í•´ëŠ” ë³´ìƒí•˜ì§€ ì•„ë‹ˆí•©ë‹ˆë‹¤.
1. ê³„ì•½ì, í”¼ë³´í—˜ì ë˜ëŠ” ì´ë“¤ì˜ ë²•ì •ëŒ€ë¦¬ì¸ì˜ ê³ ì˜ ë˜ëŠ” ì¤‘ëŒ€í•œ ê³¼ì‹¤ë¡œ ìƒê¸´ ì†í•´
2. ì „ìŸ, í˜ëª…, ë‚´ë€, ì‚¬ë³€, í­ë™, ì†Œìš” ë° ì´ì™€ ìœ ì‚¬í•œ ì‚¬íƒœë¡œ ìƒê¸´ ì†í•´
3. ì§€ì§„, ë¶„í™” ë“± ì²œì¬ì§€ë³€ìœ¼ë¡œ ìƒê¸´ ì†í•´

ì œ3ì¡°(ìê¸°ë¶€ë‹´ê¸ˆ) íšŒì‚¬ê°€ ë³´ìƒí•  ì†í•´ì•¡ì—ì„œ ì¦ê¶Œì— ê¸°ì¬ëœ ìê¸°ë¶€ë‹´ê¸ˆì„ ê³µì œí•˜ê³  ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•©ë‹ˆë‹¤.

ì œ11ì¡°(ë³´ìƒí•˜ëŠ” ì†í•´) íšŒì‚¬ëŠ” ì´ ê³„ì•½ì— ë”°ë¼ ë³´í—˜ì˜ ëª©ì ì— ëŒ€í•˜ì—¬ ë‹¤ìŒ ê°í˜¸ì˜ ì†í•´ë¥¼ ë³´ìƒí•©ë‹ˆë‹¤.
1. ì§ì ‘ì†í•´: í™”ì¬, ë‚™ë¢°, íŒŒì—´ ë˜ëŠ” í­ë°œë¡œ ë³´í—˜ì˜ ëª©ì ì— ìƒê¸´ ì†í•´ë¥¼ ë§í•©ë‹ˆë‹¤.
2. ì†Œë°©ì†í•´: í™”ì¬ë¥¼ ì†Œë°©í•˜ê¸° ìœ„í•˜ì—¬ í•„ìš”í•œ ì¡°ì¹˜ë¡œ ìƒê¸´ ì†í•´ë¥¼ ë§í•©ë‹ˆë‹¤.
3. í”¼ë‚œì†í•´: í™”ì¬ê°€ ë°œìƒí•œ ë•Œ í”¼ë‚œìœ¼ë¡œ ìƒê¸´ ë³´í—˜ì˜ ëª©ì ì˜ ì†í•´ë¥¼ ë§í•©ë‹ˆë‹¤.
4. ì”ì¡´ë¬¼ ì œê±°ë¹„ìš©: ì†í•´ë¥¼ ì…ì€ ë³´í—˜ì˜ ëª©ì ì˜ ì”ì¡´ë¬¼ì„ ì œê±°í•˜ëŠ”ë° ë“œëŠ” ë¹„ìš©
5. ì†í•´ë°©ì§€ë¹„ìš©: ì†í•´ì˜ ë°©ì§€ ë˜ëŠ” ê²½ê°ì„ ìœ„í•˜ì—¬ ì§€ì¶œí•œ í•„ìš” ë˜ëŠ” ìœ ìµí•œ ë¹„ìš©

ë‹¤ë§Œ, ê³„ì•½ì, í”¼ë³´í—˜ì ë˜ëŠ” ì´ë“¤ì˜ ë²•ì •ëŒ€ë¦¬ì¸ì˜ ê³ ì˜ ë˜ëŠ” ì¤‘ëŒ€í•œ ê³¼ì‹¤ë¡œ ìƒê¸´ ì†í•´ëŠ” ë³´ìƒí•˜ì§€ ì•„ë‹ˆí•©ë‹ˆë‹¤.
"""


def main():
    print("\n" + "="*80)
    print("ğŸ“„ ë³´í—˜ì•½ê´€ íŒŒì‹± ë° ì²­í‚¹ í…ŒìŠ¤íŠ¸ (Neo4j ì—†ì´)")
    print("="*80)
    
    # Step 1: Extract clauses
    print("\n[Step 1] ì¡°í•­ ì¶”ì¶œ ì¤‘...")
    extractor = ClauseExtractor()
    clauses = extractor.extract_clauses(SAMPLE_TEXT)
    
    # Extract items for each clause
    for clause in clauses:
        extractor.extract_items_from_clause(clause)
    
    print(f"âœ… {len(clauses)}ê°œ ì¡°í•­ ì¶”ì¶œ ì™„ë£Œ")
    
    # Display clauses
    print("\n" + "-"*80)
    print("ì¶”ì¶œëœ ì¡°í•­:")
    print("-"*80)
    for i, clause in enumerate(clauses, 1):
        print(f"\n{i}. {clause.clause_id} - {clause.title}")
        print(f"   íƒ€ì… íŒíŠ¸: {clause.clause_type or 'None (LLMì´ ê²°ì •)'}")
        print(f"   ì„¹ì…˜ ê²½ë¡œ: {clause.section_path}")
        if clause.parent_section:
            print(f"   ìƒìœ„ íŠ¹ì•½: {clause.parent_section}")
        print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(clause.full_text)}ì")
        if clause.items:
            print(f"   í•­ëª© ìˆ˜: {len(clause.items)}ê°œ")
            for j, item in enumerate(clause.items[:2], 1):  # Show first 2 items
                print(f"     {j}. {item[:80]}...")
    
    # Step 2: Test semantic chunking
    print("\n" + "="*80)
    print("[Step 2] LLM ê¸°ë°˜ ì„¸ë¯¸-ì‹œë§¨í‹± ì²­í‚¹ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    try:
        chunker = SemanticChunker()
        
        # Test with one clause (use longest one for better demo)
        test_clause = max(clauses, key=lambda c: len(c.full_text))
        print(f"\ní…ŒìŠ¤íŠ¸ ì¡°í•­: {test_clause.clause_id} - {test_clause.title}")
        print(f"ì›ë³¸ í…ìŠ¤íŠ¸:\n{test_clause.full_text[:200]}...")
        
        metadata = {
            'clause_id': test_clause.clause_id,
            'title': test_clause.title,
            'clause_type': test_clause.clause_type
        }
        
        print("\nğŸ¤– LLM ë¶„ì„ ì¤‘...")
        chunks = chunker.chunk_text(test_clause.full_text, metadata)
        
        print(f"\nâœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        
        print("\n" + "-"*80)
        print("LLMì´ ì‹ë³„í•œ Semantic Chunks:")
        print("-"*80)
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\n[Chunk {i}] {chunk.semantic_type.upper()}")
            print(f"  ë¼ë²¨: {chunk.metadata.get('label', 'N/A')}")
            print(f"  ê·¼ê±°: {chunk.metadata.get('reasoning', 'N/A')}")
            print(f"  LLM ì‹ë³„: {chunk.metadata.get('llm_identified', False)}")
            print(f"  ë‚´ìš©: {chunk.content[:150]}...")
        
    except Exception as e:
        print(f"\nâš ï¸  LLM ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("   (OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”)")
    
    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)
    
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. Dockerë¥¼ ì‹œì‘í•˜ê³  Neo4j ì‹¤í–‰: cd docker && docker-compose up -d")
    print("2. ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”: python scripts/init_schema.py")
    print("3. ì•½ê´€ ì¸ì œìŠ¤ì…˜: python scripts/ingest_policy.py --file <PDFê²½ë¡œ> ...")
    print("4. QA í…ŒìŠ¤íŠ¸: python scripts/test_qa.py")


if __name__ == "__main__":
    main()

