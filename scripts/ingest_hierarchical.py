#!/usr/bin/env python3
"""
ê³„ì¸µì  ì¡°-í•­-í˜¸ êµ¬ì¡°ë¡œ PDF ë³´í—˜ì•½ê´€ Ingestion

- Article (ì¡°)
- Paragraph (í•­)
- Item (í˜¸)
- ê° ë ˆë²¨ë³„ ì„ë² ë”©
- ì¡°í•­ ê°„ ìƒí˜¸ ì°¸ì¡° ê´€ê³„
"""
import sys
from pathlib import Path
import json

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

import argparse
from loguru import logger
from openai import OpenAI
from neo4j import GraphDatabase

from src.config.settings import settings
from src.parsers.pdf_parser import PolicyPDFParser
from src.parsers.clause_extractor import ClauseExtractor


def update_progress(progress_file: str, stage: str, percent: int, detail: str = ""):
    """Update progress file for frontend"""
    if progress_file:
        try:
            with open(progress_file, 'w') as f:
                json.dump({
                    "stage": stage,
                    "percent": percent,
                    "detail": detail
                }, f)
        except:
            pass


def main():
    parser = argparse.ArgumentParser(description="Ingest PDF with hierarchical structure (ì¡°-í•­-í˜¸)")
    parser.add_argument("--pdf", type=str, required=True, help="Path to PDF file")
    parser.add_argument("--product-code", type=str, required=True, help="Product code")
    parser.add_argument("--product-name", type=str, required=True, help="Product name")
    parser.add_argument("--version-id", type=str, required=True, help="Version ID")
    parser.add_argument("--max-clauses", type=int, default=None, help="Max clauses to process")
    parser.add_argument("--progress-file", type=str, default=None, help="Progress file path for status updates")
    
    args = parser.parse_args()
    progress_file = args.progress_file
    
    pdf_file = Path(args.pdf)
    product_code = args.product_code
    product_name = args.product_name
    version_id = args.version_id
    max_clauses = args.max_clauses
    
    if not pdf_file.exists():
        logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}")
        return
    
    logger.info("="*80)
    logger.info("ğŸ“¦ ê³„ì¸µì  PDF Ingestion ì‹œì‘ (ì¡°-í•­-í˜¸)")
    logger.info("="*80)
    logger.info(f"PDF: {pdf_file.name}")
    logger.info(f"Product: {product_name} ({product_code})")
    logger.info(f"Version: {version_id}")
    if max_clauses:
        logger.info(f"Max Clauses: {max_clauses}")
    logger.info("="*80)
    
    # Initialize clients
    openai_client = OpenAI(api_key=settings.openai_api_key)
    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_username, settings.neo4j_password)
    )
    
    stats = {
        'pdf_file': pdf_file.name,
        'pages': 0,
        'total_clauses': 0,
        'processed_clauses': 0,
        'paragraphs': 0,
        'items': 0,
        'cross_references': 0,
        'embeddings': 0,
        'nodes_created': 0
    }
    
    try:
        # Step 1: Parse PDF
        logger.info("\n[Step 1] PDF íŒŒì‹± ì¤‘...")
        update_progress(progress_file, "PDF íŒŒì‹± ì¤‘...", 5, f"íŒŒì¼: {pdf_file.name}")
        
        pdf_parser = PolicyPDFParser(str(pdf_file))
        pages = pdf_parser.extract_text_by_page()
        text = pdf_parser.extract_full_text()
        stats['pages'] = len(pages)
        logger.info(f"  âœ… {stats['pages']}í˜ì´ì§€ ì¶”ì¶œ")
        update_progress(progress_file, "PDF íŒŒì‹± ì™„ë£Œ", 10, f"{stats['pages']}í˜ì´ì§€ ì¶”ì¶œë¨")
        
        # Step 2: Extract clauses
        logger.info("\n[Step 2] ì¡°í•­ ì¶”ì¶œ ì¤‘...")
        update_progress(progress_file, "ì¡°í•­ ì¶”ì¶œ ì¤‘...", 12, "ì•½ê´€ì—ì„œ ì¡°í•­ êµ¬ì¡° ë¶„ì„ ì¤‘")
        
        extractor = ClauseExtractor()
        all_clauses = extractor.extract_clauses(text)
        stats['total_clauses'] = len(all_clauses)
        
        # Limit clauses if needed
        clauses = all_clauses[:max_clauses] if max_clauses else all_clauses
        stats['processed_clauses'] = len(clauses)
        logger.info(f"  âœ… ì²˜ë¦¬í•  ì¡°í•­: {stats['processed_clauses']}ê°œ")
        update_progress(progress_file, "ì¡°í•­ ì¶”ì¶œ ì™„ë£Œ", 15, f"ì´ {stats['processed_clauses']}ê°œ ì¡°í•­ ì¶”ì¶œ")
        
        # Step 3: Extract paragraphs and items
        logger.info("\n[Step 3] í•­(é …)ê³¼ í˜¸(è™Ÿ) ì¶”ì¶œ ì¤‘...")
        
        all_paragraphs = []
        all_items = []
        
        for idx, clause in enumerate(clauses, 1):
            update_progress(progress_file, "í•­/í˜¸ ì¶”ì¶œ ì¤‘...", 15 + int((idx / len(clauses)) * 7), 
                           f"ì¡°í•­ {idx}/{len(clauses)} - {clause.clause_id}")
            paragraphs, items = extractor.extract_paragraphs_and_items(clause)
            all_paragraphs.extend(paragraphs)
            all_items.extend(items)
        
        stats['paragraphs'] = len(all_paragraphs)
        stats['items'] = len(all_items)
        logger.info(f"  âœ… í•­: {stats['paragraphs']}ê°œ, í˜¸: {stats['items']}ê°œ")
        update_progress(progress_file, "í•­/í˜¸ ì¶”ì¶œ ì™„ë£Œ", 22, f"í•­ {stats['paragraphs']}ê°œ, í˜¸ {stats['items']}ê°œ ì¶”ì¶œ")
        
        # Step 4: Find cross-references (from Paragraphs and Items)
        logger.info("\n[Step 4] í•­/í˜¸ ê°„ ìƒí˜¸ ì°¸ì¡° íƒìƒ‰ ì¤‘...")
        update_progress(progress_file, "ìƒí˜¸ ì°¸ì¡° ë¶„ì„ ì¤‘...", 25, "ì¡°í•­ ê°„ ì°¸ì¡° ê´€ê³„ íƒìƒ‰")
        all_references = []
        
        # Find references from Paragraphs
        for paragraph in all_paragraphs:
            refs = extractor.find_cross_references(paragraph.text)
            for ref in refs:
                all_references.append({
                    'from_id': paragraph.paragraph_id,
                    'from_type': 'paragraph',
                    'to_id': ref['to'],
                    'to_type': ref['type']
                })
        
        # Find references from Items
        for item in all_items:
            refs = extractor.find_cross_references(item.text)
            for ref in refs:
                all_references.append({
                    'from_id': item.item_id,
                    'from_type': 'item',
                    'to_id': ref['to'],
                    'to_type': ref['type']
                })
        
        stats['cross_references'] = len(all_references)
        logger.info(f"  âœ… {stats['cross_references']}ê°œ ìƒí˜¸ ì°¸ì¡° ë°œê²¬")
        update_progress(progress_file, "ìƒí˜¸ ì°¸ì¡° ë¶„ì„ ì™„ë£Œ", 30, f"{stats['cross_references']}ê°œ ìƒí˜¸ ì°¸ì¡° ë°œê²¬")
        
        # Step 5: Generate embeddings
        logger.info("\n[Step 5] ì„ë² ë”© ìƒì„± ì¤‘...")
        total_items_to_embed = len(clauses) + len(all_paragraphs) + len(all_items)
        embedded_count = 0
        
        # Clause embeddings
        logger.info(f"  [5.1] ì¡°(æ¢) ì„ë² ë”© ìƒì„± ì¤‘... ({len(clauses)}ê°œ)")
        for i, clause in enumerate(clauses, 1):
            update_progress(progress_file, "ì¡°(æ¢) ì„ë² ë”© ìƒì„± ì¤‘...", 30 + int((embedded_count / total_items_to_embed) * 30), 
                           f"ì¡°í•­ {i}/{len(clauses)} - {clause.clause_id}")
            try:
                response = openai_client.embeddings.create(
                    model=settings.embedding_model,
                    input=clause.full_text
                )
                clause.embedding = response.data[0].embedding
                stats['embeddings'] += 1
                embedded_count += 1
            except Exception as e:
                logger.warning(f"  âœ— ì¡°í•­ {clause.clause_id} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                embedded_count += 1
        
        logger.info(f"  âœ… {len(clauses)}ê°œ ì¡°í•­ ì„ë² ë”© ì™„ë£Œ")
        
        # Paragraph embeddings
        logger.info(f"  [5.2] í•­(é …) ì„ë² ë”© ìƒì„± ì¤‘... ({len(all_paragraphs)}ê°œ)")
        for i, paragraph in enumerate(all_paragraphs, 1):
            update_progress(progress_file, "í•­(é …) ì„ë² ë”© ìƒì„± ì¤‘...", 30 + int((embedded_count / total_items_to_embed) * 30), 
                           f"í•­ {i}/{len(all_paragraphs)} - {paragraph.paragraph_id}")
            try:
                response = openai_client.embeddings.create(
                    model=settings.embedding_model,
                    input=paragraph.text
                )
                paragraph.embedding = response.data[0].embedding
                stats['embeddings'] += 1
                embedded_count += 1
            except Exception as e:
                logger.warning(f"  âœ— í•­ {paragraph.paragraph_id} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                embedded_count += 1
        
        logger.info(f"  âœ… {len(all_paragraphs)}ê°œ í•­ ì„ë² ë”© ì™„ë£Œ")
        
        # Item embeddings
        logger.info(f"  [5.3] í˜¸(è™Ÿ) ì„ë² ë”© ìƒì„± ì¤‘... ({len(all_items)}ê°œ)")
        for i, item in enumerate(all_items, 1):
            update_progress(progress_file, "í˜¸(è™Ÿ) ì„ë² ë”© ìƒì„± ì¤‘...", 30 + int((embedded_count / total_items_to_embed) * 30), 
                           f"í˜¸ {i}/{len(all_items)} - {item.item_id}")
            try:
                response = openai_client.embeddings.create(
                    model=settings.embedding_model,
                    input=item.text
                )
                item.embedding = response.data[0].embedding
                stats['embeddings'] += 1
                embedded_count += 1
            except Exception as e:
                logger.warning(f"  âœ— í˜¸ {item.item_id} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                embedded_count += 1
        
        logger.info(f"  âœ… {len(all_items)}ê°œ í˜¸ ì„ë² ë”© ì™„ë£Œ")
        logger.info(f"  âœ… ì´ {stats['embeddings']}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        update_progress(progress_file, "ì„ë² ë”© ìƒì„± ì™„ë£Œ", 60, f"ì´ {stats['embeddings']}ê°œ ì„ë² ë”© ìƒì„±")
        
        # Step 6: Load to Neo4j
        logger.info("\n[Step 6] Neo4jì— ë¡œë”© ì¤‘...")
        update_progress(progress_file, "Neo4j ë¡œë”© ì‹œì‘...", 62, "ìƒí’ˆ ë° ë²„ì „ ìƒì„± ì¤‘")
        
        with driver.session() as session:
            # Create product
            logger.info(f"  ìƒí’ˆ ìƒì„±: {product_name}")
            session.run("""
                MERGE (prod:InsuranceProduct {code: $code})
                SET prod.name = $name,
                    prod.kind = 'personal_injury',
                    prod.lineOfBusiness = 'personal'
                """,
                code=product_code,
                name=product_name
            )
            
            # Create version
            logger.info(f"  ë²„ì „ ìƒì„±: {version_id}")
            session.run("""
                MATCH (prod:InsuranceProduct {code: $product_code})
                MERGE (ver:PolicyVersion {versionId: $version_id})
                SET ver.effectiveFrom = date(),
                    ver.documentUrl = $pdf_path
                MERGE (prod)-[:HAS_POLICY_VERSION]->(ver)
                """,
                product_code=product_code,
                version_id=version_id,
                pdf_path=str(pdf_file)
            )
            
            # Create articles (ì¡°)
            logger.info(f"  ì¡°(æ¢) ë¡œë”©: {len(clauses)}ê°œ")
            update_progress(progress_file, "ì¡°(æ¢) ë…¸ë“œ ìƒì„± ì¤‘...", 65, f"0/{len(clauses)} ì¡° ì €ì¥ë¨")
            for idx, clause in enumerate(clauses, 1):
                update_progress(progress_file, "ì¡°(æ¢) ë…¸ë“œ ìƒì„± ì¤‘...", 65 + int((idx / len(clauses)) * 5), 
                               f"{idx}/{len(clauses)} - {clause.clause_id}")
                session.run("""
                    MATCH (ver:PolicyVersion {versionId: $version_id})
                    MERGE (a:Article {articleId: $article_id})
                    SET a.number = $number,
                        a.title = $title,
                        a.text = $text,
                        a.clauseType = $clause_type,
                        a.embedding = $embedding
                    MERGE (ver)-[:HAS_ARTICLE]->(a)
                    """,
                    version_id=version_id,
                    article_id=clause.clause_id,
                    number=clause.article_number,
                    title=clause.title,
                    text=clause.full_text,
                    clause_type=clause.clause_type or 'General',
                    embedding=clause.embedding if hasattr(clause, 'embedding') else None
                )
                stats['nodes_created'] += 1
            
            logger.info(f"  âœ… {len(clauses)}ê°œ ì¡° ë¡œë”© ì™„ë£Œ")
            update_progress(progress_file, "ì¡°(æ¢) ë…¸ë“œ ìƒì„± ì™„ë£Œ", 70, f"{len(clauses)}ê°œ ì¡° ì €ì¥ë¨")
            
            # Create paragraphs (í•­)
            logger.info(f"  í•­(é …) ë¡œë”©: {len(all_paragraphs)}ê°œ")
            for idx, paragraph in enumerate(all_paragraphs, 1):
                update_progress(progress_file, "í•­(é …) ë…¸ë“œ ìƒì„± ì¤‘...", 70 + int((idx / max(len(all_paragraphs), 1)) * 5), 
                               f"{idx}/{len(all_paragraphs)} - {paragraph.paragraph_id}")
                session.run("""
                    MATCH (a:Article {articleId: $parent_article})
                    MERGE (p:Paragraph {paragraphId: $paragraph_id})
                    SET p.number = $number,
                        p.text = $text,
                        p.embedding = $embedding
                    MERGE (a)-[:HAS_PARAGRAPH]->(p)
                    """,
                    parent_article=paragraph.parent_clause,
                    paragraph_id=paragraph.paragraph_id,
                    number=paragraph.number,
                    text=paragraph.text,
                    embedding=paragraph.embedding if hasattr(paragraph, 'embedding') else None
                )
                stats['nodes_created'] += 1
            
            logger.info(f"  âœ… {len(all_paragraphs)}ê°œ í•­ ë¡œë”© ì™„ë£Œ")
            update_progress(progress_file, "í•­(é …) ë…¸ë“œ ìƒì„± ì™„ë£Œ", 75, f"{len(all_paragraphs)}ê°œ í•­ ì €ì¥ë¨")
            
            # Create items (í˜¸)
            logger.info(f"  í˜¸(è™Ÿ) ë¡œë”©: {len(all_items)}ê°œ")
            for i, item in enumerate(all_items, 1):
                update_progress(progress_file, "í˜¸(è™Ÿ) ë…¸ë“œ ìƒì„± ì¤‘...", 75 + int((i / max(len(all_items), 1)) * 5), 
                               f"{i}/{len(all_items)} - {item.item_id}")
                try:
                    logger.debug(f"    [{i}/{len(all_items)}] {item.item_id} -> {item.parent_paragraph}")
                    session.run("""
                        MATCH (p:Paragraph {paragraphId: $parent_paragraph})
                        MERGE (i:Item {itemId: $item_id})
                        SET i.number = $number,
                            i.text = $text,
                            i.embedding = $embedding
                        MERGE (p)-[:HAS_ITEM]->(i)
                        """,
                        parent_paragraph=item.parent_paragraph,
                        item_id=item.item_id,
                        number=item.number,
                        text=item.text,
                        embedding=item.embedding if hasattr(item, 'embedding') else None
                    )
                    stats['nodes_created'] += 1
                except Exception as e:
                    logger.error(f"  âœ— í˜¸ {item.item_id} ì €ì¥ ì‹¤íŒ¨: {e}")
            
            logger.info(f"  âœ… {len(all_items)}ê°œ í˜¸ ë¡œë”© ì™„ë£Œ")
            update_progress(progress_file, "í˜¸(è™Ÿ) ë…¸ë“œ ìƒì„± ì™„ë£Œ", 80, f"{len(all_items)}ê°œ í˜¸ ì €ì¥ë¨")
            
            # Create cross-references (Paragraph/Item â†’ Paragraph/Item/Article)
            # - êµ¬ì²´ì  í•­/í˜¸ ëª…ì‹œ: Paragraph/Itemë¡œ ì—°ê²°
            # - ì¡°ë§Œ ì–¸ê¸‰: Articleë¡œ ì—°ê²°
            logger.info(f"  ìƒí˜¸ ì°¸ì¡° ê´€ê³„ ìƒì„±: {len(all_references)}ê°œ")
            update_progress(progress_file, "ìƒí˜¸ ì°¸ì¡° ê´€ê³„ ìƒì„± ì¤‘...", 82, f"0/{len(all_references)} REFERS_TO ê´€ê³„ ìƒì„±")
            refs_created = 0
            
            for ref_idx, ref in enumerate(all_references, 1):
                update_progress(progress_file, "ìƒí˜¸ ì°¸ì¡° ê´€ê³„ ìƒì„± ì¤‘...", 
                               82 + int((ref_idx / max(len(all_references), 1)) * 8), 
                               f"{ref_idx}/{len(all_references)} - {ref['from_id']} â†’ {ref['to_id']}")
                try:
                    # Determine from_node label and property
                    if ref['from_type'] == 'paragraph':
                        from_label = 'Paragraph'
                        from_prop = 'paragraphId'
                    elif ref['from_type'] == 'item':
                        from_label = 'Item'
                        from_prop = 'itemId'
                    else:
                        continue
                    
                    # Determine to_node label and property
                    if ref['to_type'] == 'clause':
                        # ì¡°ë§Œ ì–¸ê¸‰ëœ ê²½ìš°: Articleë¡œ ì—°ê²°
                        to_label = 'Article'
                        to_prop = 'articleId'
                        to_id = ref['to_id']
                    elif ref['to_type'] == 'paragraph':
                        # êµ¬ì²´ì  í•­ ëª…ì‹œ: Paragraphë¡œ ì—°ê²°
                        to_label = 'Paragraph'
                        to_prop = 'paragraphId'
                        to_id = ref['to_id']
                    elif ref['to_type'] == 'item':
                        # êµ¬ì²´ì  í˜¸ ëª…ì‹œ: Itemë¡œ ì—°ê²°
                        to_label = 'Item'
                        to_prop = 'itemId'
                        to_id = ref['to_id']
                    else:
                        continue
                    
                    # Create REFERS_TO relationship
                    query = f"""
                        MATCH (from_node:{from_label} {{{from_prop}: $from_id}})
                        MATCH (to_node:{to_label} {{{to_prop}: $to_id}})
                        MERGE (from_node)-[:REFERS_TO]->(to_node)
                    """
                    
                    session.run(query, from_id=ref['from_id'], to_id=to_id)
                    refs_created += 1
                    logger.debug(f"    ì°¸ì¡°: {ref['from_id']} â†’ {to_id} ({to_label})")
                    
                except Exception as e:
                    logger.warning(f"  âœ— ì°¸ì¡° ê´€ê³„ ìƒì„± ì‹¤íŒ¨ ({ref['from_id']} -> {ref['to_id']}): {e}")
            
            logger.info(f"  âœ… {refs_created}ê°œ ìƒí˜¸ ì°¸ì¡° ê´€ê³„ ìƒì„± ì™„ë£Œ")
            update_progress(progress_file, "ìƒí˜¸ ì°¸ì¡° ê´€ê³„ ìƒì„± ì™„ë£Œ", 90, f"{refs_created}ê°œ REFERS_TO ê´€ê³„ ìƒì„±ë¨")
        
        # Step 7: Generate recommended queries (optional)
        update_progress(progress_file, "ì¶”ì²œ ì§ˆì˜ ìƒì„± ì¤‘...", 92, "LLMìœ¼ë¡œ ì¶”ì²œ ì§ˆì˜ ìƒì„± ì¤‘")
        
        # Summary
        update_progress(progress_file, "ì™„ë£Œ!", 100, f"ì¡°: {stats['processed_clauses']}ê°œ, í•­: {stats['paragraphs']}ê°œ, í˜¸: {stats['items']}ê°œ ì €ì¥")
        logger.info("\n" + "="*80)
        logger.info("âœ… ê³„ì¸µì  Ingestion ì™„ë£Œ!")
        logger.info("="*80)
        logger.info(f"PDF íŒŒì¼: {stats['pdf_file']}")
        logger.info(f"í˜ì´ì§€: {stats['pages']}ê°œ")
        logger.info(f"ì´ ì¡°í•­: {stats['total_clauses']}ê°œ")
        logger.info(f"ì²˜ë¦¬ëœ ì¡°í•­: {stats['processed_clauses']}ê°œ")
        logger.info(f"í•­(é …): {stats['paragraphs']}ê°œ")
        logger.info(f"í˜¸(è™Ÿ): {stats['items']}ê°œ")
        logger.info(f"ìƒí˜¸ ì°¸ì¡°: {stats['cross_references']}ê°œ")
        logger.info(f"ì„ë² ë”©: {stats['embeddings']}ê°œ")
        logger.info(f"ë…¸ë“œ ìƒì„±: {stats['nodes_created']}ê°œ")
        logger.info("="*80)
        
        # Step 8: Clean up uploaded file
        if 'uploads' in str(pdf_file):
            try:
                import os
                if pdf_file.exists():
                    os.remove(pdf_file)
                    logger.info(f"ğŸ—‘ï¸ ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œë¨: {pdf_file.name}")
            except Exception as cleanup_error:
                logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {cleanup_error}")
        
    except Exception as e:
        logger.error(f"âŒ Ingestion ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    finally:
        driver.close()


if __name__ == "__main__":
    main()

