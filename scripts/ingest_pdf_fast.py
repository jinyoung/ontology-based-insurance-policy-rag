#!/usr/bin/env python3
"""
PDF ì•½ê´€ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ Neo4jì— Ingestion (Fast Mode - ìƒìœ„ Nê°œ ì¡°í•­ë§Œ)
"""
import sys
import argparse
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from neo4j import GraphDatabase
from openai import OpenAI

from src.config.settings import settings
from src.parsers.pdf_parser import PolicyPDFParser
from src.parsers.clause_extractor import ClauseExtractor
from src.chunking.semantic_chunker import SemanticChunker


def ingest_pdf_fast(pdf_path: str,
                    product_code: str,
                    product_name: str,
                    version_id: str,
                    max_clauses: int = 30):
    """
    PDF ì•½ê´€ì„ íŒŒì‹±í•˜ì—¬ Neo4jì— ë¡œë”© (ìƒìœ„ Nê°œ ì¡°í•­ë§Œ)
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        product_code: ìƒí’ˆ ì½”ë“œ
        product_name: ìƒí’ˆëª…
        version_id: ë²„ì „ ID
        max_clauses: ì²˜ë¦¬í•  ìµœëŒ€ ì¡°í•­ ìˆ˜ (ê¸°ë³¸ 30ê°œ)
    """
    
    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        sys.exit(1)
    
    logger.info("="*80)
    logger.info(f"ğŸš€ Fast PDF Ingestion ì‹œì‘: {pdf_file.name} (ìƒìœ„ {max_clauses}ê°œ ì¡°í•­)")
    logger.info("="*80)
    
    stats = {
        'pdf_file': pdf_file.name,
        'pages': 0,
        'total_clauses': 0,
        'processed_clauses': 0,
        'clause_summaries': 0,
        'chunks': 0,
        'embeddings': 0,
        'nodes_created': 0
    }
    
    # Step 1: Parse PDF
    logger.info("\n[Step 1] PDF íŒŒì‹± ì¤‘...")
    with PolicyPDFParser(str(pdf_file)) as parser:
        full_text = parser.extract_full_text()
        metadata = parser.get_metadata()
        stats['pages'] = metadata['total_pages']
        
        logger.info(f"  âœ… {stats['pages']}í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
        logger.info(f"  ì´ {len(full_text):,}ì")
    
    # Step 2: Extract clauses
    logger.info("\n[Step 2] ì¡°í•­ ì¶”ì¶œ ì¤‘...")
    extractor = ClauseExtractor()
    all_clauses = extractor.extract_clauses(full_text)
    
    # Extract items for each clause
    for clause in all_clauses:
        extractor.extract_items_from_clause(clause)
    
    stats['total_clauses'] = len(all_clauses)
    
    # Limit to first N clauses
    clauses = all_clauses[:max_clauses]
    stats['processed_clauses'] = len(clauses)
    
    logger.info(f"  âœ… ì´ {stats['total_clauses']}ê°œ ì¡°í•­ ì¤‘ {stats['processed_clauses']}ê°œ ì²˜ë¦¬")
    
    # Show sample
    logger.info(f"\n  ì²˜ë¦¬í•  ì¡°í•­:")
    for clause in clauses[:5]:
        logger.info(f"    - {clause.clause_id}: {clause.title} ({clause.clause_type or 'General'})")
    if len(clauses) > 5:
        logger.info(f"    ... ì™¸ {len(clauses)-5}ê°œ")
    
    # Step 3: Semantic chunking
    all_chunks = []
    
    logger.info("\n[Step 3] LLM ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹ ì¤‘...")
    chunker = SemanticChunker()
    
    for i, clause in enumerate(clauses, 1):
        if len(clause.full_text) > 150:
            metadata = {
                'clause_id': clause.clause_id,
                'title': clause.title,
                'clause_type': clause.clause_type
            }
            
            logger.info(f"  [{i}/{len(clauses)}] {clause.clause_id} ì²­í‚¹ ì¤‘... ({len(clause.full_text)}ì)")
            
            try:
                chunks = chunker.chunk_text(clause.full_text, metadata)
                all_chunks.extend(chunks)
                logger.info(f"       â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            except Exception as e:
                logger.error(f"       âœ— ì²­í‚¹ ì‹¤íŒ¨: {e}")
        else:
            logger.debug(f"  [{i}/{len(clauses)}] {clause.clause_id} - í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ê±´ë„ˆëœ€")
    
    stats['chunks'] = len(all_chunks)
    logger.info(f"\n  âœ… ì´ {stats['chunks']}ê°œ ì‹œë§¨í‹± ì²­í¬ ìƒì„±")
    
    # Step 3.5: Summarize and embed clauses
    clause_summaries = []
    
    logger.info("\n[Step 3.5] ì¡°í•­ ìš”ì•½ ë° ì„ë² ë”© ìƒì„± ì¤‘...")
    openai_client = OpenAI(api_key=settings.openai_api_key)
    
    for i, clause in enumerate(clauses, 1):
        try:
            logger.info(f"  [{i}/{len(clauses)}] {clause.clause_id} ìš”ì•½ ì¤‘... ({len(clause.full_text)}ì)")
            
            # Summarize clause using LLM
            summary_response = openai_client.chat.completions.create(
                model=settings.llm_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´í—˜ì•½ê´€ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¡°í•­ì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."},
                    {"role": "user", "content": f"ë‹¤ìŒ ì¡°í•­ì„ ìš”ì•½í•˜ì„¸ìš”:\n\nì œëª©: {clause.title}\n\në‚´ìš©:\n{clause.full_text}"}
                ],
                temperature=0.3,
                max_tokens=300
            )
            
            summary = summary_response.choices[0].message.content.strip()
            logger.info(f"       â†’ ìš”ì•½ ì™„ë£Œ: {summary[:50]}...")
            
            # Generate embedding for summary
            embedding_response = openai_client.embeddings.create(
                model=settings.embedding_model,
                input=summary
            )
            embedding = embedding_response.data[0].embedding
            
            clause_summaries.append({
                'clause_id': clause.clause_id,
                'title': clause.title,
                'summary': summary,
                'embedding': embedding
            })
            
        except Exception as e:
            logger.warning(f"       âœ— ìš”ì•½/ì„ë² ë”© ì‹¤íŒ¨: {e}")
            continue
    
    stats['clause_summaries'] = len(clause_summaries)
    logger.info(f"\n  âœ… {len(clause_summaries)}ê°œ ì¡°í•­ ìš”ì•½ ë° ì„ë² ë”© ì™„ë£Œ")
    
    # Step 4: Generate embeddings for chunks
    chunks_with_embeddings = []
    
    logger.info("\n[Step 4] ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘...")
    
    for i, chunk in enumerate(all_chunks, 1):
        try:
            logger.info(f"  [{i}/{len(all_chunks)}] ì„ë² ë”© ìƒì„± ì¤‘...")
            
            response = openai_client.embeddings.create(
                model=settings.embedding_model,
                input=chunk.content
            )
            embedding = response.data[0].embedding
            
            chunks_with_embeddings.append({
                'chunk_id': chunk.chunk_id,
                'text': chunk.content,
                'embedding': embedding,
                'semantic_type': chunk.semantic_type,
                'metadata': chunk.metadata
            })
            
        except Exception as e:
            logger.warning(f"  âœ— ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            continue
    
    stats['embeddings'] = len(chunks_with_embeddings)
    logger.info(f"\n  âœ… {stats['embeddings']}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    # Step 5: Load into Neo4j
    logger.info("\n[Step 5] Neo4jì— ë¡œë”© ì¤‘...")
    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_username, settings.neo4j_password)
    )
    
    with driver.session() as session:
        # Create product
        logger.info(f"  ìƒí’ˆ ìƒì„±: {product_name} ({product_code})")
        session.run("""
            MERGE (prod:InsuranceProduct {code: $code})
            SET prod.name = $name,
                prod.kind = 'personal_injury',
                prod.lineOfBusiness = 'personal'
            """,
            code=product_code,
            name=product_name
        )
        
        # Create version
        logger.info(f"  ë²„ì „ ìƒì„±: {version_id}")
        session.run("""
            MATCH (prod:InsuranceProduct {code: $product_code})
            MERGE (ver:PolicyVersion {versionId: $version_id})
            SET ver.effectiveFrom = date(),
                ver.documentUrl = $pdf_path
            MERGE (prod)-[:HAS_POLICY_VERSION]->(ver)
            """,
            product_code=product_code,
            version_id=version_id,
            pdf_path=str(pdf_file)
        )
        
        # Create clauses with summaries and embeddings
        logger.info(f"  ì¡°í•­ ë¡œë”©: {len(clauses)}ê°œ")
        
        # Create lookup dictionary for summaries
        summary_dict = {s['clause_id']: s for s in clause_summaries}
        
        for clause in clauses:
            clause_summary_data = summary_dict.get(clause.clause_id)
            
            if clause_summary_data:
                # Clause with summary and embedding
                session.run("""
                    MATCH (ver:PolicyVersion {versionId: $version_id})
                    MERGE (c:PolicyClause {clauseId: $clause_id})
                    SET c.title = $title,
                        c.clauseType = $clause_type,
                        c.text = $text,
                        c.summary = $summary,
                        c.embedding = $embedding,
                        c.sectionPath = $section_path,
                        c.articleNumber = $article_number
                    MERGE (ver)-[:HAS_CLAUSE]->(c)
                    """,
                    version_id=version_id,
                    clause_id=clause.clause_id,
                    title=clause.title,
                    clause_type=clause.clause_type or 'General',
                    text=clause.full_text,
                    summary=clause_summary_data['summary'],
                    embedding=clause_summary_data['embedding'],
                    section_path=clause.section_path,
                    article_number=clause.article_number
                )
            else:
                # Clause without summary (fallback)
                session.run("""
                    MATCH (ver:PolicyVersion {versionId: $version_id})
                    MERGE (c:PolicyClause {clauseId: $clause_id})
                    SET c.title = $title,
                        c.clauseType = $clause_type,
                        c.text = $text,
                        c.sectionPath = $section_path,
                        c.articleNumber = $article_number
                    MERGE (ver)-[:HAS_CLAUSE]->(c)
                    """,
                    version_id=version_id,
                    clause_id=clause.clause_id,
                    title=clause.title,
                    clause_type=clause.clause_type or 'General',
                    text=clause.full_text,
                    section_path=clause.section_path,
                    article_number=clause.article_number
                )
            
            # Create special clause link
            if clause.parent_section:
                session.run("""
                    MATCH (ver:PolicyVersion {versionId: $version_id})
                    MATCH (c:PolicyClause {clauseId: $clause_id})
                    MERGE (sc:SpecialClause {name: $special_clause_name})
                    MERGE (ver)-[:HAS_SPECIAL_CLAUSE]->(sc)
                    MERGE (sc)-[:HAS_CLAUSE]->(c)
                    """,
                    version_id=version_id,
                    clause_id=clause.clause_id,
                    special_clause_name=clause.parent_section
                )
        
        logger.info(f"  âœ… ì¡°í•­ ë¡œë”© ì™„ë£Œ")
        
        # Create chunks
        if chunks_with_embeddings:
            logger.info(f"  ì²­í¬ ë¡œë”©: {len(chunks_with_embeddings)}ê°œ")
            
            for i, chunk_data in enumerate(chunks_with_embeddings, 1):
                parent_clause_id = chunk_data['metadata'].get('clause_id')
                
                if parent_clause_id:
                    try:
                        session.run("""
                            MATCH (c:PolicyClause {clauseId: $parent_clause_id})
                            CREATE (p:ParagraphChunk {
                                chunkId: $chunk_id,
                                text: $text,
                                semanticType: $semantic_type,
                                embedding: $embedding
                            })
                            CREATE (c)-[:HAS_PARAGRAPH]->(p)
                            """,
                            parent_clause_id=parent_clause_id,
                            chunk_id=chunk_data['chunk_id'],
                            text=chunk_data['text'],
                            semantic_type=chunk_data['semantic_type'],
                            embedding=chunk_data['embedding']
                        )
                        
                        # Create Coverage/Exclusion nodes
                        if chunk_data['semantic_type'] == 'coverage':
                            session.run("""
                                MATCH (p:ParagraphChunk {chunkId: $chunk_id})
                                MERGE (cov:Coverage {
                                    code: $code,
                                    name: $name
                                })
                                MERGE (p)-[:DEFINES_COVERAGE]->(cov)
                                """,
                                chunk_id=chunk_data['chunk_id'],
                                code=f"COV_{chunk_data['chunk_id']}",
                                name=chunk_data['text'][:50]
                            )
                        elif chunk_data['semantic_type'] == 'exclusion':
                            session.run("""
                                MATCH (p:ParagraphChunk {chunkId: $chunk_id})
                                MERGE (exc:Exclusion {
                                    code: $code,
                                    description: $description
                                })
                                MERGE (p)-[:HAS_EXCLUSION]->(exc)
                                """,
                                chunk_id=chunk_data['chunk_id'],
                                code=f"EXC_{chunk_data['chunk_id']}",
                                description=chunk_data['text'][:50]
                            )
                        
                    except Exception as e:
                        logger.warning(f"  âœ— ì²­í¬ {i} ë¡œë”© ì‹¤íŒ¨: {e}")
            
            logger.info(f"  âœ… ì²­í¬ ë¡œë”© ì™„ë£Œ")
        
        # Verify
        result = session.run("""
            MATCH (ver:PolicyVersion {versionId: $version_id})
            OPTIONAL MATCH (ver)-[:HAS_CLAUSE]->(c:PolicyClause)
            OPTIONAL MATCH (c)-[:HAS_PARAGRAPH]->(p:ParagraphChunk)
            RETURN count(DISTINCT c) as clauses, count(DISTINCT p) as chunks
            """,
            version_id=version_id
        )
        
        record = result.single()
        stats['nodes_created'] = record['clauses'] + record['chunks']
    
    driver.close()
    
    # Final summary
    logger.info("\n" + "="*80)
    logger.info("âœ… Fast Ingestion ì™„ë£Œ!")
    logger.info("="*80)
    logger.info(f"PDF íŒŒì¼: {stats['pdf_file']}")
    logger.info(f"í˜ì´ì§€: {stats['pages']}ê°œ")
    logger.info(f"ì´ ì¡°í•­: {stats['total_clauses']}ê°œ")
    logger.info(f"ì²˜ë¦¬ëœ ì¡°í•­: {stats['processed_clauses']}ê°œ")
    logger.info(f"ì¡°í•­ ìš”ì•½: {stats['clause_summaries']}ê°œ")
    logger.info(f"ì²­í¬: {stats['chunks']}ê°œ")
    logger.info(f"ì²­í¬ ì„ë² ë”©: {stats['embeddings']}ê°œ")
    logger.info(f"ë…¸ë“œ ìƒì„±: {stats['nodes_created']}ê°œ")
    logger.info("="*80)
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="ë³´í—˜ì•½ê´€ PDFë¥¼ íŒŒì‹±í•˜ì—¬ Neo4jì— Fast Ingestion (ìƒìœ„ Nê°œ ì¡°í•­)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ìƒìœ„ 30ê°œ ì¡°í•­ë§Œ ì²˜ë¦¬
  python scripts/ingest_pdf_fast.py \\
    --pdf data/raw/20120401_15101_1.pdf \\
    --product-code LIG_PERSONAL_INJURY_2007 \\
    --product-name "LIG ê°œì¸ìƒí•´ë³´í—˜" \\
    --version-id LIG_PERSONAL_INJURY_2007_V1 \\
    --max-clauses 30
    
  # ìƒìœ„ 50ê°œ ì¡°í•­ ì²˜ë¦¬
  python scripts/ingest_pdf_fast.py \\
    --pdf data/raw/20120401_15101_1.pdf \\
    --product-code LIG_PERSONAL_INJURY_2007 \\
    --product-name "LIG ê°œì¸ìƒí•´ë³´í—˜" \\
    --version-id LIG_PERSONAL_INJURY_2007_V1 \\
    --max-clauses 50
        """
    )
    
    parser.add_argument(
        '--pdf',
        required=True,
        help='PDF íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--product-code',
        required=True,
        help='ìƒí’ˆ ì½”ë“œ (ì˜ˆ: LIG_PERSONAL_INJURY_2007)'
    )
    parser.add_argument(
        '--product-name',
        required=True,
        help='ìƒí’ˆëª… (ì˜ˆ: "LIG ê°œì¸ìƒí•´ë³´í—˜")'
    )
    parser.add_argument(
        '--version-id',
        required=True,
        help='ë²„ì „ ID (ì˜ˆ: LIG_PERSONAL_INJURY_2007_V1)'
    )
    parser.add_argument(
        '--max-clauses',
        type=int,
        default=30,
        help='ì²˜ë¦¬í•  ìµœëŒ€ ì¡°í•­ ìˆ˜ (ê¸°ë³¸: 30ê°œ)'
    )
    
    args = parser.parse_args()
    
    try:
        stats = ingest_pdf_fast(
            pdf_path=args.pdf,
            product_code=args.product_code,
            product_name=args.product_name,
            version_id=args.version_id,
            max_clauses=args.max_clauses
        )
        
        logger.info("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info(f"  python3 test_api_client.py")
        logger.info(f"\n  ë˜ëŠ” curl:")
        logger.info(f"  curl -X POST http://localhost:8001/api/v1/query \\")
        logger.info(f"    -H 'Content-Type: application/json' \\")
        logger.info(f"    -d '{{\"question\": \"ë³´ìƒí•˜ëŠ” ì†í•´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"}}'")
        
    except Exception as e:
        logger.error(f"\nâŒ Ingestion ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

