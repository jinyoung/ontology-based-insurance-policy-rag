#!/usr/bin/env python3
"""
PDF ì•½ê´€ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ Neo4jì— Ingestion
"""
import sys
import argparse
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from neo4j import GraphDatabase
from openai import OpenAI

from src.config.settings import settings
from src.parsers.pdf_parser import PolicyPDFParser
from src.parsers.clause_extractor import ClauseExtractor
from src.chunking.semantic_chunker import SemanticChunker


def ingest_pdf(pdf_path: str,
               product_code: str,
               product_name: str,
               version_id: str,
               use_semantic_chunking: bool = True):
    """
    PDF ì•½ê´€ì„ íŒŒì‹±í•˜ì—¬ Neo4jì— ë¡œë”©
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        product_code: ìƒí’ˆ ì½”ë“œ
        product_name: ìƒí’ˆëª…
        version_id: ë²„ì „ ID
        use_semantic_chunking: LLM ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€
    """
    
    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        sys.exit(1)
    
    logger.info("="*80)
    logger.info(f"ğŸš€ PDF Ingestion ì‹œì‘: {pdf_file.name}")
    logger.info("="*80)
    
    stats = {
        'pdf_file': pdf_file.name,
        'pages': 0,
        'clauses': 0,
        'chunks': 0,
        'embeddings': 0,
        'nodes_created': 0
    }
    
    # Step 1: Parse PDF
    logger.info("\n[Step 1] PDF íŒŒì‹± ì¤‘...")
    with PolicyPDFParser(str(pdf_file)) as parser:
        full_text = parser.extract_full_text()
        metadata = parser.get_metadata()
        stats['pages'] = metadata['total_pages']
        
        logger.info(f"  âœ… {stats['pages']}í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
        logger.info(f"  ì´ {len(full_text):,}ì")
    
    # Step 2: Extract clauses
    logger.info("\n[Step 2] ì¡°í•­ ì¶”ì¶œ ì¤‘...")
    extractor = ClauseExtractor()
    clauses = extractor.extract_clauses(full_text)
    
    # Extract items for each clause
    for clause in clauses:
        extractor.extract_items_from_clause(clause)
    
    stats['clauses'] = len(clauses)
    logger.info(f"  âœ… {stats['clauses']}ê°œ ì¡°í•­ ì¶”ì¶œ ì™„ë£Œ")
    
    # Show sample
    logger.info(f"\n  ìƒ˜í”Œ ì¡°í•­:")
    for clause in clauses[:3]:
        logger.info(f"    - {clause.clause_id}: {clause.title} ({clause.clause_type or 'General'})")
    if len(clauses) > 3:
        logger.info(f"    ... ì™¸ {len(clauses)-3}ê°œ")
    
    # Step 3: Semantic chunking
    all_chunks = []
    
    if use_semantic_chunking:
        logger.info("\n[Step 3] LLM ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹ ì¤‘...")
        chunker = SemanticChunker()
        
        for i, clause in enumerate(clauses, 1):
            if len(clause.full_text) > 150:
                metadata = {
                    'clause_id': clause.clause_id,
                    'title': clause.title,
                    'clause_type': clause.clause_type
                }
                
                logger.info(f"  [{i}/{len(clauses)}] {clause.clause_id} ì²­í‚¹ ì¤‘... ({len(clause.full_text)}ì)")
                
                try:
                    chunks = chunker.chunk_text(clause.full_text, metadata)
                    all_chunks.extend(chunks)
                    logger.info(f"       â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                except Exception as e:
                    logger.error(f"       âœ— ì²­í‚¹ ì‹¤íŒ¨: {e}")
            else:
                logger.debug(f"  [{i}/{len(clauses)}] {clause.clause_id} - í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ê±´ë„ˆëœ€")
        
        stats['chunks'] = len(all_chunks)
        logger.info(f"\n  âœ… ì´ {stats['chunks']}ê°œ ì‹œë§¨í‹± ì²­í¬ ìƒì„±")
    else:
        logger.info("\n[Step 3] ì‹œë§¨í‹± ì²­í‚¹ ê±´ë„ˆëœ€ (--no-semantic-chunking)")
        stats['chunks'] = 0
    
    # Step 4: Generate embeddings
    chunks_with_embeddings = []
    
    if all_chunks:
        logger.info("\n[Step 4] ì„ë² ë”© ìƒì„± ì¤‘...")
        openai_client = OpenAI(api_key=settings.openai_api_key)
        
        for i, chunk in enumerate(all_chunks, 1):
            try:
                logger.info(f"  [{i}/{len(all_chunks)}] ì„ë² ë”© ìƒì„± ì¤‘...")
                
                response = openai_client.embeddings.create(
                    model=settings.embedding_model,
                    input=chunk.content
                )
                embedding = response.data[0].embedding
                
                chunks_with_embeddings.append({
                    'chunk_id': chunk.chunk_id,
                    'text': chunk.content,
                    'embedding': embedding,
                    'semantic_type': chunk.semantic_type,
                    'metadata': chunk.metadata
                })
                
            except Exception as e:
                logger.warning(f"  âœ— ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                continue
        
        stats['embeddings'] = len(chunks_with_embeddings)
        logger.info(f"\n  âœ… {stats['embeddings']}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    else:
        logger.info("\n[Step 4] ì²­í¬ê°€ ì—†ì–´ ì„ë² ë”© ìƒì„± ê±´ë„ˆëœ€")
    
    # Step 5: Load into Neo4j
    logger.info("\n[Step 5] Neo4jì— ë¡œë”© ì¤‘...")
    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_username, settings.neo4j_password)
    )
    
    with driver.session() as session:
        # Create product
        logger.info(f"  ìƒí’ˆ ìƒì„±: {product_name} ({product_code})")
        session.run("""
            MERGE (prod:InsuranceProduct {code: $code})
            SET prod.name = $name,
                prod.kind = 'property',
                prod.lineOfBusiness = 'personal'
            """,
            code=product_code,
            name=product_name
        )
        
        # Create version
        logger.info(f"  ë²„ì „ ìƒì„±: {version_id}")
        session.run("""
            MATCH (prod:InsuranceProduct {code: $product_code})
            MERGE (ver:PolicyVersion {versionId: $version_id})
            SET ver.effectiveFrom = date(),
                ver.documentUrl = $pdf_path
            MERGE (prod)-[:HAS_POLICY_VERSION]->(ver)
            """,
            product_code=product_code,
            version_id=version_id,
            pdf_path=str(pdf_file)
        )
        
        # Create clauses
        logger.info(f"  ì¡°í•­ ë¡œë”©: {len(clauses)}ê°œ")
        for clause in clauses:
            session.run("""
                MATCH (ver:PolicyVersion {versionId: $version_id})
                MERGE (c:PolicyClause {clauseId: $clause_id})
                SET c.title = $title,
                    c.clauseType = $clause_type,
                    c.text = $text,
                    c.sectionPath = $section_path,
                    c.articleNumber = $article_number
                MERGE (ver)-[:HAS_CLAUSE]->(c)
                """,
                version_id=version_id,
                clause_id=clause.clause_id,
                title=clause.title,
                clause_type=clause.clause_type or 'General',
                text=clause.full_text,
                section_path=clause.section_path,
                article_number=clause.article_number
            )
            
            # Create special clause link
            if clause.parent_section:
                session.run("""
                    MATCH (ver:PolicyVersion {versionId: $version_id})
                    MATCH (c:PolicyClause {clauseId: $clause_id})
                    MERGE (sc:SpecialClause {name: $special_clause_name})
                    MERGE (ver)-[:HAS_SPECIAL_CLAUSE]->(sc)
                    MERGE (sc)-[:HAS_CLAUSE]->(c)
                    """,
                    version_id=version_id,
                    clause_id=clause.clause_id,
                    special_clause_name=clause.parent_section
                )
        
        logger.info(f"  âœ… ì¡°í•­ ë¡œë”© ì™„ë£Œ")
        
        # Create chunks
        if chunks_with_embeddings:
            logger.info(f"  ì²­í¬ ë¡œë”©: {len(chunks_with_embeddings)}ê°œ")
            
            for i, chunk_data in enumerate(chunks_with_embeddings, 1):
                parent_clause_id = chunk_data['metadata'].get('clause_id')
                
                if parent_clause_id:
                    try:
                        session.run("""
                            MATCH (c:PolicyClause {clauseId: $parent_clause_id})
                            CREATE (p:ParagraphChunk {
                                chunkId: $chunk_id,
                                text: $text,
                                semanticType: $semantic_type,
                                embedding: $embedding
                            })
                            CREATE (c)-[:HAS_PARAGRAPH]->(p)
                            """,
                            parent_clause_id=parent_clause_id,
                            chunk_id=chunk_data['chunk_id'],
                            text=chunk_data['text'],
                            semantic_type=chunk_data['semantic_type'],
                            embedding=chunk_data['embedding']
                        )
                        
                        # Create Coverage/Exclusion nodes
                        if chunk_data['semantic_type'] == 'coverage':
                            session.run("""
                                MATCH (p:ParagraphChunk {chunkId: $chunk_id})
                                MERGE (cov:Coverage {
                                    code: $code,
                                    name: $name
                                })
                                MERGE (p)-[:DEFINES_COVERAGE]->(cov)
                                """,
                                chunk_id=chunk_data['chunk_id'],
                                code=f"COV_{chunk_data['chunk_id']}",
                                name=chunk_data['text'][:50]
                            )
                        elif chunk_data['semantic_type'] == 'exclusion':
                            session.run("""
                                MATCH (p:ParagraphChunk {chunkId: $chunk_id})
                                MERGE (exc:Exclusion {
                                    code: $code,
                                    description: $description
                                })
                                MERGE (p)-[:HAS_EXCLUSION]->(exc)
                                """,
                                chunk_id=chunk_data['chunk_id'],
                                code=f"EXC_{chunk_data['chunk_id']}",
                                description=chunk_data['text'][:50]
                            )
                        
                    except Exception as e:
                        logger.warning(f"  âœ— ì²­í¬ {i} ë¡œë”© ì‹¤íŒ¨: {e}")
            
            logger.info(f"  âœ… ì²­í¬ ë¡œë”© ì™„ë£Œ")
        
        # Verify
        result = session.run("""
            MATCH (ver:PolicyVersion {versionId: $version_id})
            OPTIONAL MATCH (ver)-[:HAS_CLAUSE]->(c:PolicyClause)
            OPTIONAL MATCH (c)-[:HAS_PARAGRAPH]->(p:ParagraphChunk)
            RETURN count(DISTINCT c) as clauses, count(DISTINCT p) as chunks
            """,
            version_id=version_id
        )
        
        record = result.single()
        stats['nodes_created'] = record['clauses'] + record['chunks']
    
    driver.close()
    
    # Final summary
    logger.info("\n" + "="*80)
    logger.info("âœ… Ingestion ì™„ë£Œ!")
    logger.info("="*80)
    logger.info(f"PDF íŒŒì¼: {stats['pdf_file']}")
    logger.info(f"í˜ì´ì§€: {stats['pages']}ê°œ")
    logger.info(f"ì¡°í•­: {stats['clauses']}ê°œ")
    logger.info(f"ì²­í¬: {stats['chunks']}ê°œ")
    logger.info(f"ì„ë² ë”©: {stats['embeddings']}ê°œ")
    logger.info(f"ë…¸ë“œ ìƒì„±: {stats['nodes_created']}ê°œ")
    logger.info("="*80)
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="ë³´í—˜ì•½ê´€ PDFë¥¼ íŒŒì‹±í•˜ì—¬ Neo4jì— Ingestion",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/ingest_pdf.py \\
    --pdf data/raw/LIG_ì£¼íƒí™”ì¬ë³´í—˜.pdf \\
    --product-code LIG_HOME_FIRE_2025 \\
    --product-name "LIG ì£¼íƒí™”ì¬ë³´í—˜" \\
    --version-id LIG_HOME_FIRE_2025_V1
        """
    )
    
    parser.add_argument(
        '--pdf',
        required=True,
        help='PDF íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--product-code',
        required=True,
        help='ìƒí’ˆ ì½”ë“œ (ì˜ˆ: LIG_HOME_FIRE_2025)'
    )
    parser.add_argument(
        '--product-name',
        required=True,
        help='ìƒí’ˆëª… (ì˜ˆ: "LIG ì£¼íƒí™”ì¬ë³´í—˜")'
    )
    parser.add_argument(
        '--version-id',
        required=True,
        help='ë²„ì „ ID (ì˜ˆ: LIG_HOME_FIRE_2025_V1)'
    )
    parser.add_argument(
        '--no-semantic-chunking',
        action='store_true',
        help='LLM ì‹œë§¨í‹± ì²­í‚¹ ë¹„í™œì„±í™” (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)'
    )
    
    args = parser.parse_args()
    
    try:
        stats = ingest_pdf(
            pdf_path=args.pdf,
            product_code=args.product_code,
            product_name=args.product_name,
            version_id=args.version_id,
            use_semantic_chunking=not args.no_semantic_chunking
        )
        
        logger.info("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info(f"  python3 test_api_client.py")
        logger.info(f"  curl -X POST http://localhost:8001/api/v1/query \\")
        logger.info(f"    -H 'Content-Type: application/json' \\")
        logger.info(f"    -d '{{\"question\": \"ë³´ìƒí•˜ëŠ” ì†í•´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"}}'")
        
    except Exception as e:
        logger.error(f"\nâŒ Ingestion ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

